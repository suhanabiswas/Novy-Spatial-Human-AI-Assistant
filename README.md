**Exploring User Interactions with Spatially-Aware AI Assistants: A Comparison in Gaussian splat-based and Mesh-based Virtual environments**

This research investigates how users interact with spatially-aware AI assistants in virtual reality (VR), focusing on the interplay between advanced rendering (Gaussian splatting), multimodal input (voice + gesture), and human-AI collaboration. While recent advances in both photorealistic scene rendering and large language model (LLM)-based assistants have significantly expanded the technical possibilities for immersive systems, there remains a notable gap in understanding how users engage with these technologies, particularly in the context of spatial design and manipulation tasks. The study compares two rendering techniques, Gaussian splats and traditional mesh-based environments, and two input modalities: standard VR controllers versus a voice-and-gesture-driven AI assistant. Gaussian splatting enables high-fidelity, photo-realistic VR scenes but poses interaction challenges due to its lack of physical mesh structures. To address this, the system introduces simplified colliders that preserve visual quality while enabling precise object manipulation. A central component of the system is a spatially and context-aware AI assistant capable of interpreting natural deictic commands like “put this here” or “make this bigger,” using both voice input and pointing gestures. This AI assistant leverages embodied spatial context to interpret user intent, allowing for more intuitive interactions. This aims to bridge the well-known expectation-capability gap between users and intelligent systems. Through a user study, this research evaluates how different combinations of input modality and scene representation impact task performance, user satisfaction, cognitive workload (measured via NASA-TLX), and interaction preferences. Participants perform common spatial tasks such as adding, arranging, and modifying virtual objects in a small office layout, providing both objective performance data and subjective feedback. By systematically exploring the intersection of interaction modality, AI assistance, and rendering fidelity, this research contributes insights into the design of intelligent, accessible, and user-aligned VR systems. Findings aim to inform future applications in collaborative virtual environments, educational tools, and digital twin systems where intuitive, spatially grounded AI interaction is critical for usability and productivity. Technical challenges being faced: One of the primary technical challenges in this system is handling simultaneous multimodal input, particularly when users issue spatial commands like “put this here” while pointing. For accurate interpretation, the system must temporally align the speech (“this” and “here”) with the user’s corresponding pointing gestures, both toward an object and a destination. Even small mismatches in timing due to latency in voice processing or gesture tracking can lead to incorrect results, disrupting the interaction flow. A second challenge is interpreting egocentric spatial references, such as “move this to the left.” Here, “left” must be resolved relative to the user’s current viewpoint, not the object’s local coordinate system or a fixed world axis. Ensuring this alignment, especially as the user turns their head or body, is non-trivial and requires real-time spatial context awareness. Another difficulty lies in accurately identifying pointing targets: since users’ hands naturally move, even small jitters can affect ray- or cone-cast-based selection, leading to ambiguity about the intended destination when they say “here.” Lastly, prompt engineering for the LLM-based assistant presents ongoing challenges. Prompts must be carefully crafted and dynamically adapted to elicit structured and spatially accurate responses. Yet, the AI’s outputs can still be inconsistent, requiring the system to handle deviations gracefully and enforce spatial correctness even when the LLM’s response is imperfect.


The branches:
1.  Branch azure works without wake word with azure api. UI is the same as older one.
2.	Branch azure-wake-word uses azure and needs hey novy everytime to send the command, no need for confirmation with "yes, send" anymore!
3.	Branch openai-whisper1 uses openai api, needs no wake word or confirmation for now. Send whenever user says something.

In branches azure-wake-word and openai-whisper1:
gpt4-azure-api: Uses Azure API to connect to gpt4.1 model
gpt4-openai-api: Uses OpenAI API to connect to gpt4o model
Currently using NAudio to access microphone of the headset and to record voice commands from user. This was done to avoid the lag related issues reported with Unity's Microphone.Start() function.
